{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import summarize\n",
    "import sentences\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X.AUTHID</th>\n",
       "      <th>anger_doc_sum</th>\n",
       "      <th>anticipation_doc_sum</th>\n",
       "      <th>arousal_doc_sum</th>\n",
       "      <th>disgust_doc_sum</th>\n",
       "      <th>dominance_doc_sum</th>\n",
       "      <th>fear_doc_sum</th>\n",
       "      <th>joy_doc_sum</th>\n",
       "      <th>neg_doc_sum</th>\n",
       "      <th>num_text_words</th>\n",
       "      <th>...</th>\n",
       "      <th>cSIN_label_fin</th>\n",
       "      <th>cEXC_label_fin</th>\n",
       "      <th>cCOM_label_fin</th>\n",
       "      <th>cRUG_label_fin</th>\n",
       "      <th>cSOP_label_fin</th>\n",
       "      <th>cSIN_fin_rank</th>\n",
       "      <th>cEXC_fin_rank</th>\n",
       "      <th>cCOM_fin_rank</th>\n",
       "      <th>cRUG_fin_rank</th>\n",
       "      <th>cSOP_fin_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'http://blog.searsholdings.com/leadership-vie...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>31.317</td>\n",
       "      <td>0</td>\n",
       "      <td>38.293</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>124</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'https://atyourservice.blogs.xerox.com/catego...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23.260</td>\n",
       "      <td>0</td>\n",
       "      <td>32.665</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            X.AUTHID  anger_doc_sum  \\\n",
       "0  b'http://blog.searsholdings.com/leadership-vie...              1   \n",
       "1  b'https://atyourservice.blogs.xerox.com/catego...              0   \n",
       "\n",
       "   anticipation_doc_sum  arousal_doc_sum  disgust_doc_sum  dominance_doc_sum  \\\n",
       "0                     9           31.317                0             38.293   \n",
       "1                     4           23.260                0             32.665   \n",
       "\n",
       "   fear_doc_sum  joy_doc_sum  neg_doc_sum  num_text_words  ...  \\\n",
       "0             7           16            7             124  ...   \n",
       "1             0            1            2             126  ...   \n",
       "\n",
       "   cSIN_label_fin  cEXC_label_fin cCOM_label_fin cRUG_label_fin  \\\n",
       "0               n               y              y              n   \n",
       "1               y               n              y              n   \n",
       "\n",
       "   cSOP_label_fin  cSIN_fin_rank  cEXC_fin_rank  cCOM_fin_rank  cRUG_fin_rank  \\\n",
       "0               y            5.0            1.0            2.0            3.0   \n",
       "1               y            NaN            NaN            NaN            NaN   \n",
       "\n",
       "  cSOP_fin_rank  \n",
       "0           4.0  \n",
       "1           NaN  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Data\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process into documents\n",
    "site_content = data['site.content']\n",
    "docs = []\n",
    "for i in range(site_content.size):\n",
    "    docs.append(eval(site_content[i]).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into sentences\n",
    "sent_split_docs = []\n",
    "for doc in docs:\n",
    "    sent_split_docs.append(nltk.sent_tokenize(doc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################"
     ]
    }
   ],
   "source": [
    "## Feature based summarisation\n",
    "\n",
    "feature_base_sents=[]\n",
    "for d,doc in enumerate(sent_split_docs):\n",
    "    \n",
    "    filter_doc = []\n",
    "    for s,sent in enumerate(doc):\n",
    "        filter_sent = sentences.filtersent(sent, remove_stopwords=True , stem=False, lemm=True, avoid_single_char=True)\n",
    "        # Avoid null strings --> They cause problems\n",
    "        if filter_sent == '':\n",
    "            del doc[s]\n",
    "        else:\n",
    "            filter_doc.append(filter_sent)\n",
    "        \n",
    "    # feature summarise the passage\n",
    "    select_indices = summarize.feature_summ(filter_doc, mode='sents', sent_lim=3, lam=0.7, mu=0.85)\n",
    "    feature_base_sents.append([])\n",
    "    for idx,selection in enumerate(select_indices):\n",
    "        if selection==1:\n",
    "            feature_base_sents[d].append(doc[idx])\n",
    "\n",
    "    # Log progress\n",
    "    if( d%10 == 0):\n",
    "        print('#', end='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################"
     ]
    }
   ],
   "source": [
    "## Topic based summarisation\n",
    "\n",
    "LSA_Gong_Liu_sents=[]\n",
    "LSA_Stein_sents=[]\n",
    "# LSA_Stein_MMR_Greed_sents=[]\n",
    "LSA_Stein_MMR_ILP_sents=[]\n",
    "LSA_Murray_sents=[]\n",
    "LSA_Murray_CM_sents=[]\n",
    "for d,doc in enumerate(sent_split_docs):\n",
    "    \n",
    "    filter_doc = []\n",
    "    for s,sent in enumerate(doc):\n",
    "        filter_sent = sentences.filtersent(sent, remove_stopwords=True , stem=True, lemm=True, avoid_single_char=True)\n",
    "        # Avoid null strings --> They cause problems\n",
    "        if filter_sent == '':\n",
    "            del doc[s]\n",
    "        else:\n",
    "            filter_doc.append(filter_sent)\n",
    "        \n",
    "    # LSA Gong Liu topic summarise the passage\n",
    "    select_indices = summarize.LSA_summ(filter_doc, selection='gong', sent_lim=3, mu=0.85, lam=0.7, cross_method=False)\n",
    "    LSA_Gong_Liu_sents.append([])\n",
    "    for idx,selection in enumerate(select_indices):\n",
    "        if selection==1:\n",
    "            LSA_Gong_Liu_sents[d].append(doc[idx])\n",
    "            \n",
    "    # LSA Stein topic summarise the passage\n",
    "    select_indices = summarize.LSA_summ(filter_doc, selection='stein', sent_lim=3, mu=0.85, lam=0.7, cross_method=False)\n",
    "    LSA_Stein_sents.append([])\n",
    "    for idx,selection in enumerate(select_indices):\n",
    "        if selection==1:\n",
    "            LSA_Stein_sents[d].append(doc[idx])\n",
    "            \n",
    "    # LSA Stein MMR ILP topic summarise the passage\n",
    "    select_indices = summarize.LSA_summ(filter_doc, selection='stein-ILP', sent_lim=3, mu=0.85, lam=0.7, cross_method=False)\n",
    "    LSA_Stein_MMR_ILP_sents.append([])\n",
    "    for idx,selection in enumerate(select_indices):\n",
    "        if selection==1:\n",
    "            LSA_Stein_MMR_ILP_sents[d].append(doc[idx])\n",
    "            \n",
    "    # LSA Murray topic summarise the passage\n",
    "    select_indices = summarize.LSA_summ(filter_doc, selection='murray', sent_lim=3, mu=0.85, lam=0.7, cross_method=False)\n",
    "    LSA_Murray_sents.append([])\n",
    "    for idx,selection in enumerate(select_indices):\n",
    "        if selection==1:\n",
    "            LSA_Murray_sents[d].append(doc[idx])\n",
    "            \n",
    "    # LSA Murray topic summarise the passage\n",
    "    select_indices = summarize.LSA_summ(filter_doc, selection='murray', sent_lim=3, mu=0.85, lam=0.7, cross_method=True)\n",
    "    LSA_Murray_sents.append([])\n",
    "    for idx,selection in enumerate(select_indices):\n",
    "        if selection==1:\n",
    "            LSA_Murray_sents[d].append(doc[idx])\n",
    "            \n",
    "    ### You can try other possibilities by enabling or disabling the cross method on different selection methods\n",
    "\n",
    "    # Log progress\n",
    "    if( d%10 == 0):\n",
    "        print('#', end='')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Take, for e, the devices that make up SMB document management infrastructure inclusive of printers, scanners, copiers and that serve SMB document processing needs.',\n",
       " 'But beyond just day-to-day device management, with managed print services, these devices communicate back to an operations center when critical supplies are needed to keep documents flowing within the organization.',\n",
       " 'In addition to device management, ConnectKey opens document infrastructure to mobile users, allowing them to print and access data from their smartphones or tablets.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
