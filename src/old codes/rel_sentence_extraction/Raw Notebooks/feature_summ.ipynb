{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "from ortools.linear_solver import pywraplp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(743, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X.AUTHID</th>\n",
       "      <th>anger_doc_sum</th>\n",
       "      <th>anticipation_doc_sum</th>\n",
       "      <th>arousal_doc_sum</th>\n",
       "      <th>disgust_doc_sum</th>\n",
       "      <th>dominance_doc_sum</th>\n",
       "      <th>fear_doc_sum</th>\n",
       "      <th>joy_doc_sum</th>\n",
       "      <th>neg_doc_sum</th>\n",
       "      <th>num_text_words</th>\n",
       "      <th>...</th>\n",
       "      <th>cSIN_label_fin</th>\n",
       "      <th>cEXC_label_fin</th>\n",
       "      <th>cCOM_label_fin</th>\n",
       "      <th>cRUG_label_fin</th>\n",
       "      <th>cSOP_label_fin</th>\n",
       "      <th>cSIN_fin_rank</th>\n",
       "      <th>cEXC_fin_rank</th>\n",
       "      <th>cCOM_fin_rank</th>\n",
       "      <th>cRUG_fin_rank</th>\n",
       "      <th>cSOP_fin_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'http://blog.searsholdings.com/leadership-vie...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>31.317</td>\n",
       "      <td>0</td>\n",
       "      <td>38.293</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>124</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'https://atyourservice.blogs.xerox.com/catego...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23.260</td>\n",
       "      <td>0</td>\n",
       "      <td>32.665</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            X.AUTHID  anger_doc_sum  \\\n",
       "0  b'http://blog.searsholdings.com/leadership-vie...              1   \n",
       "1  b'https://atyourservice.blogs.xerox.com/catego...              0   \n",
       "\n",
       "   anticipation_doc_sum  arousal_doc_sum  disgust_doc_sum  dominance_doc_sum  \\\n",
       "0                     9           31.317                0             38.293   \n",
       "1                     4           23.260                0             32.665   \n",
       "\n",
       "   fear_doc_sum  joy_doc_sum  neg_doc_sum  num_text_words  ...  \\\n",
       "0             7           16            7             124  ...   \n",
       "1             0            1            2             126  ...   \n",
       "\n",
       "   cSIN_label_fin  cEXC_label_fin cCOM_label_fin cRUG_label_fin  \\\n",
       "0               n               y              y              n   \n",
       "1               y               n              y              n   \n",
       "\n",
       "   cSOP_label_fin  cSIN_fin_rank  cEXC_fin_rank  cCOM_fin_rank  cRUG_fin_rank  \\\n",
       "0               y            5.0            1.0            2.0            3.0   \n",
       "1               y            NaN            NaN            NaN            NaN   \n",
       "\n",
       "  cSOP_fin_rank  \n",
       "0           4.0  \n",
       "1           NaN  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "print(data.shape)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_content = data['site.content']\n",
    "docs = {}\n",
    "for i in range(site_content.size):\n",
    "    docs[i] = eval(site_content[i]).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = [1]*len(docs)\n",
    "corpus = [1]*len(docs)\n",
    "copy_corpus = [1]*len(docs)\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    original_corpus[i] = sent_tokenize(docs[i])\n",
    "    corpus[i] = sent_tokenize(docs[i])\n",
    "    copy_corpus[i] = sent_tokenize(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "remove_punctuation_map = dict((ord(char), ' ') for char in string.punctuation)\n",
    "integer_chars = [str(digit) for digit in list(range(10))]\n",
    "remove_integer_map = dict((ord(char), None) for char in integer_chars)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def lemm_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(item) for item in tokens]\n",
    "\n",
    "def filtersent(text, remove_stopwords=True, stem=True, lemm=True, avoid_single_char=True):\n",
    "    # remove puctuations and lower the case\n",
    "    simpletext = text.lower().translate(remove_punctuation_map).translate(remove_integer_map)\n",
    "    # tokenize\n",
    "    words = nltk.word_tokenize(simpletext)\n",
    "    # remove stop words\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in stopwords.words('english')]\n",
    "    # lemmatize them\n",
    "    if lemm:\n",
    "        words = lemm_tokens(words)\n",
    "    # stem them\n",
    "    if stem:\n",
    "        words = stem_tokens(words)\n",
    "    # avoid single character words\n",
    "    if avoid_single_char:\n",
    "        words = [w for w in words if w not in string.ascii_lowercase]\n",
    "    # detikenise the sentence\n",
    "    return TreebankWordDetokenizer().detokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    for j in range(len(corpus[i])):\n",
    "        corpus[i][j] = filtersent(original_corpus[i][j])\n",
    "        copy_corpus[i][j] = original_corpus[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    idx = []\n",
    "    for j in range(len(corpus[i])):\n",
    "        if(corpus[i][j] == ''):\n",
    "            idx.insert(0,j)\n",
    "    for index in idx:\n",
    "        del corpus[i][index]\n",
    "        del copy_corpus[i][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix\n",
    "count_word = [1]*len(corpus)\n",
    "count_matrix = [1]*len(corpus)\n",
    "count_persent = [1]*len(corpus) # no of words per sentence\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "for i in range(len(corpus)):\n",
    "    count_word[i] = vectorizer.fit_transform(corpus[i])\n",
    "    count_matrix[i] = count_word[i].toarray()\n",
    "    count_persent[i] = np.sum(count_matrix[i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_score(pos):\n",
    "    if(pos<=0.1):\n",
    "        return 0.17\n",
    "    elif(pos<=0.2):\n",
    "        return 0.23\n",
    "    elif(pos<=0.3):\n",
    "        return 0.14\n",
    "    elif(pos<=0.4):\n",
    "        return 0.08\n",
    "    elif(pos<=0.5):\n",
    "        return 0.05\n",
    "    elif(pos<=0.6):\n",
    "        return 0.04\n",
    "    elif(pos<=0.7):\n",
    "        return 0.06\n",
    "    elif(pos<=0.8):\n",
    "        return 0.04\n",
    "    elif(pos<=0.9):\n",
    "        return 0.04\n",
    "    else:\n",
    "        return 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_score(pos_tag):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fam_score(fam):\n",
    "    if(fam==0):\n",
    "        return 1\n",
    "    return 1/(1+np.exp( -8 * (-0.5 + 1/fam) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.20.30.40.50.60.70.80.90.100.110.120.130.140.150.160.170.180.190.200.210.220.230.240.250.260.270.280.290.300.310.320.330.340.350.360.370.380.390.400.410.420.430.440.450.460.470.480.490.500.510.520.530.540.550.560.570.580.590.600.610.620.630.640.650.660.670.680.690.700.710.720.730.740."
     ]
    }
   ],
   "source": [
    "mu = 0.85\n",
    "lam = 0.7\n",
    "k = 3\n",
    "\n",
    "top_corpus = [1]*len(corpus)\n",
    "\n",
    "for d in range(len(corpus)):\n",
    "# if(True):\n",
    "#     d = 1\n",
    "    \n",
    "    no_sent = len(corpus[d])\n",
    "    document = TreebankWordDetokenizer().detokenize([string for string in corpus[d]])\n",
    "    \n",
    "    words = nltk.tokenize.word_tokenize(document)\n",
    "    fdist1 = nltk.FreqDist(words)\n",
    "\n",
    "#      tf word\n",
    "    tf_word = {word:freq for word, freq in fdist1.items() if not word.isdigit()}\n",
    "    \n",
    "    # length of word\n",
    "    len_word = {word: len(word) for word in fdist1.keys()}\n",
    "    \n",
    "    # pos tag\n",
    "    pos_word = {word:pos_score(nltk.pos_tag([word])[0][1]) for word in fdist1.keys()}\n",
    "    \n",
    "    # familiarity\n",
    "    fam_word = {word: fam_score(len(wn.synsets(word))) for word in fdist1.keys()}\n",
    "    \n",
    "    score_sents = []\n",
    "    for i in range(no_sent):\n",
    "        score_sent = 0\n",
    "        \n",
    "        for j,word in enumerate(nltk.tokenize.word_tokenize(corpus[d][i])):\n",
    "            # occurence score\n",
    "            occ_word = position_score(j/len(corpus[d][i]))\n",
    "            \n",
    "            score_word = tf_word[word] * len_word[word] * pos_word[word] * fam_word[word] * occ_word\n",
    "            score_sent += score_word\n",
    "            \n",
    "            # corefferant\n",
    "            if(pos_word[word]=='PRP'):\n",
    "                if(j/len(corpus[s][i]) < 0.5):\n",
    "                    if(i>0):\n",
    "                        score_sent += score_sents[i-1]/len(nltk.tokenize.word_tokenize(corpus[d][i-1]))\n",
    "                else:\n",
    "                    score_sent += score_sent/len(nltk.tokenize.word_tokenize(corpus[d][i]))\n",
    "        \n",
    "        score_sents.append(score_sent)\n",
    "    \n",
    "    # similarity matrix\n",
    "    sim = np.zeros((no_sent, no_sent))\n",
    "    worthless_sent = []\n",
    "    for x in range(no_sent):\n",
    "        for y in range(no_sent):\n",
    "            adotb = np.dot(count_matrix[d][x], count_matrix[d][y])\n",
    "            a = LA.norm(count_matrix[d][x])\n",
    "            b = LA.norm(count_matrix[d][y])\n",
    "            if(x==y):\n",
    "                sim[x,y] = (1-mu)/no_sent\n",
    "            else:\n",
    "                sim[x,y] = mu*(adotb/(a*b)) + (1-mu)/no_sent\n",
    "    \n",
    "    denom = np.sum(sim,axis=1)\n",
    "    for x in range(no_sent):\n",
    "        sim[x,:] = sim[x,:] * 1/denom[x]\n",
    "        \n",
    "    # Markov's process\n",
    "    v = np.array(score_sents)\n",
    "    \n",
    "    while(True):\n",
    "        v_dash = v.dot(sim)\n",
    "        if(LA.norm(v_dash - v) < 0.000000001):\n",
    "            break\n",
    "        v = v_dash\n",
    "    \n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    redundacy = sim;\n",
    "\n",
    "    # ILP selection\n",
    "    solver = pywraplp.Solver('SolveAssignmentProblemMIP', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n",
    "\n",
    "    for i in range(no_sent):\n",
    "        alpha[i] = solver.BoolVar('alpha[%i]' % i)\n",
    "\n",
    "    for i in range(no_sent):\n",
    "        for j in range(i+1,no_sent):\n",
    "            beta[i,j] = solver.BoolVar('beta[%i,%i]' %(i,j))\n",
    "\n",
    "\n",
    "    solver.Maximize(solver.Sum(alpha[i]*lam*v[i] for i in range(no_sent)) - solver.Sum([beta[i,j]*(1-lam)*redundacy[i,j] for j in range(i+1,no_sent) for i in range(no_sent)]))\n",
    "\n",
    "    # constraints\n",
    "    solver.Add( solver.Sum([alpha[i] for i in range(no_sent)]) <= k )\n",
    "\n",
    "    for i in range(no_sent):\n",
    "        for j in range(i+1,no_sent):\n",
    "            solver.Add( beta[i,j] <= alpha[i])\n",
    "            solver.Add( beta[i,j] <= alpha[j])\n",
    "            solver.Add( alpha[i] + alpha[j] <= 1 + beta[i,j])\n",
    "\n",
    "    sol = solver.Solve()\n",
    "    \n",
    "    alpha_sol = [alpha[i].solution_value() for i in range(no_sent)]\n",
    "    top_sents = []\n",
    "    for i in range(no_sent):\n",
    "        if alpha_sol[i]==1:\n",
    "            top_sents.append(copy_corpus[d][i])\n",
    "    top_corpus[d] = top_sents\n",
    "    \n",
    "    if(d%10 == 0):\n",
    "        print(d,end=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X.AUTHID</th>\n",
       "      <th>FEA1</th>\n",
       "      <th>FEA2</th>\n",
       "      <th>FEA3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'http://blog.searsholdings.com/leadership-vie...</td>\n",
       "      <td>SHC Speaks by As Chief HR Officer and co-e spo...</td>\n",
       "      <td>This year, SHC partnered with , and to marry f...</td>\n",
       "      <td>I was joined by dozens of SHC associates and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'https://atyourservice.blogs.xerox.com/catego...</td>\n",
       "      <td>When you look at your Xerox device and see the...</td>\n",
       "      <td>Recently new support videos were posted to the...</td>\n",
       "      <td>Xerox and Xerox and Design are trademarks of X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'http://blogs.zebra.com/whitepaper-label'</td>\n",
       "      <td>; March 17, 2015 at 8:03 AM ;Product identifi...</td>\n",
       "      <td>It is critical that the labels you utilize are...</td>\n",
       "      <td>;In this paper, Zebra provides guidance on how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'https://connect.blogs.xerox.com/author/lauri...</td>\n",
       "      <td>Writer and PR Consultant Email LinkedIn Twitte...</td>\n",
       "      <td>By By Laurie Riedman, PR Consultant Throughout...</td>\n",
       "      <td>Xerox and Xerox and Design are trademarks of X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'http://blog.searsholdings.com/2016/07/page/2/'</td>\n",
       "      <td>For our Danbury Sears store, this meant that w...</td>\n",
       "      <td>On Saturday, July 2, Sears celebrated the gran...</td>\n",
       "      <td>More than 20 Sears associates volunteered with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            X.AUTHID  \\\n",
       "0  b'http://blog.searsholdings.com/leadership-vie...   \n",
       "1  b'https://atyourservice.blogs.xerox.com/catego...   \n",
       "2         b'http://blogs.zebra.com/whitepaper-label'   \n",
       "3  b'https://connect.blogs.xerox.com/author/lauri...   \n",
       "4   b'http://blog.searsholdings.com/2016/07/page/2/'   \n",
       "\n",
       "                                                FEA1  \\\n",
       "0  SHC Speaks by As Chief HR Officer and co-e spo...   \n",
       "1  When you look at your Xerox device and see the...   \n",
       "2   ; March 17, 2015 at 8:03 AM ;Product identifi...   \n",
       "3  Writer and PR Consultant Email LinkedIn Twitte...   \n",
       "4  For our Danbury Sears store, this meant that w...   \n",
       "\n",
       "                                                FEA2  \\\n",
       "0  This year, SHC partnered with , and to marry f...   \n",
       "1  Recently new support videos were posted to the...   \n",
       "2  It is critical that the labels you utilize are...   \n",
       "3  By By Laurie Riedman, PR Consultant Throughout...   \n",
       "4  On Saturday, July 2, Sears celebrated the gran...   \n",
       "\n",
       "                                                FEA3  \n",
       "0  I was joined by dozens of SHC associates and f...  \n",
       "1  Xerox and Xerox and Design are trademarks of X...  \n",
       "2  ;In this paper, Zebra provides guidance on how...  \n",
       "3  Xerox and Xerox and Design are trademarks of X...  \n",
       "4  More than 20 Sears associates volunteered with...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data = pd.DataFrame({\"X.AUTHID\": data[\"X.AUTHID\"],\n",
    "                        \"FEA1\": [ s[0] for s in top_corpus],\n",
    "                         \"FEA2\": [ s[1] for s in top_corpus],\n",
    "                         \"FEA3\": [ s[2] for s in top_corpus]})\n",
    "top_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ill be hosting a unique set of customersat ;, SYN109, (Wednesday May 24, 4:45pm) where well have a discussion and audience-driven Q&A with 4 Citrix Cloud customers:;Well cut the marketing fluff.',\n",
       " \"Just candid, matter-of-fact discussions about:;Ken is Sr. Director of Product Marketing in Citrix's Cloud Services group, and spends most of his time helping lead the charge to converge concepts of cloud computing, virtual workspaces, and hosted service providers.\",\n",
       " 'One of the key differentiators ; Citrix VP of WW Sales and Strategy, Ash Vijayakanthan shares his thoughts on the benefits Citrix Cloud brings for Citrix Service Providers ; NetScaler Management and Analytics from Citrix allows administrators to easily view, automate, and manage all their application infrastructure, as well ;Dont worry, you can unsubscribe at anytime.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_corpus[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abacus</td>\n",
       "      <td>trust</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word emotion  score\n",
       "0   abacus   trust      1\n",
       "1  abandon    fear      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAD_data = pd.read_csv(\"Lexicons/NRC_VAD_Lexicon_new2.csv\")\n",
    "LEX_data = pd.read_csv(\"Lexicons/NRC_lexicon_new2.csv\")\n",
    "\n",
    "LEX_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_word = {VAD_data.iloc[i,0]:np.sum(VAD_data.iloc[i,1:]) for i in range(len(VAD_data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = set(LEX_data.iloc[:,0])\n",
    "LEX_word = {word:0 for word in word_dict}\n",
    "for word in LEX_data.iloc[:,0]:\n",
    "    LEX_word[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEX_word[\"unpaid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.20.30.40.50.60.70.80.90.100.110.120.130.140.150.160.170.180.190.200.210.220.230.240.250.260.270.280.290.300.310.320.330.340.350.360.370.380.390.400.410.420.430.440.450.460.470.480.490.500.510.520.530.540.550.560.570.580.590.600.610.620.630.640.650.660.670.680.690.700.710.720.730.740."
     ]
    }
   ],
   "source": [
    "mu = 0.85\n",
    "lam = 0.7\n",
    "k = 3\n",
    "smooth_const = 0.5\n",
    "\n",
    "top_corpus = [1]*len(corpus)\n",
    "\n",
    "for d in range(len(corpus)):\n",
    "# if(True):\n",
    "#     d = 0\n",
    "    \n",
    "    no_sent = len(corpus[d])\n",
    "    document = TreebankWordDetokenizer().detokenize([string for string in corpus[d]])\n",
    "    \n",
    "    words = nltk.tokenize.word_tokenize(document)\n",
    "    fdist1 = nltk.FreqDist(words)\n",
    "\n",
    "#      tf word\n",
    "    tf_word = {word:freq for word, freq in fdist1.items() if not word.isdigit()}\n",
    "    \n",
    "    # length of word\n",
    "    len_word = {word: len(word) for word in fdist1.keys()}\n",
    "    \n",
    "    # pos tag\n",
    "    pos_word = {word:pos_score(nltk.pos_tag([word])[0][1]) for word in fdist1.keys()}\n",
    "    \n",
    "    # familiarity\n",
    "    fam_word = {word: fam_score(len(wn.synsets(word))) for word in fdist1.keys()}\n",
    "    \n",
    "    # Lexiscores\n",
    "    lex_word = {word:smooth_const for word in fdist1.keys()}\n",
    "    for word in lex_word.keys():\n",
    "        if(word in LEX_word.keys()):\n",
    "            lex_word[word] += LEX_word[word]\n",
    "        if(word in VAD_word.keys()):\n",
    "            lex_word[word] += VAD_word[word]\n",
    "    \n",
    "    score_sents = []\n",
    "    for i in range(no_sent):\n",
    "        score_sent = 0\n",
    "        \n",
    "        for j,word in enumerate(nltk.tokenize.word_tokenize(corpus[d][i])):\n",
    "            # occurence score\n",
    "            occ_word = position_score(j/len(corpus[d][i]))\n",
    "            \n",
    "            score_word = tf_word[word] * len_word[word] * pos_word[word] * fam_word[word] * lex_word[word] * occ_word\n",
    "            score_sent += score_word\n",
    "            \n",
    "            # corefferant\n",
    "            if(pos_word[word]=='PRP'):\n",
    "                if(j/len(corpus[s][i]) < 0.5):\n",
    "                    if(i>0):\n",
    "                        score_sent += score_sents[i-1]/len(nltk.tokenize.word_tokenize(corpus[d][i-1]))\n",
    "                else:\n",
    "                    score_sent += score_sent/len(nltk.tokenize.word_tokenize(corpus[d][i]))\n",
    "        \n",
    "        score_sents.append(score_sent)\n",
    "    \n",
    "    # similarity matrix\n",
    "    sim = np.zeros((no_sent, no_sent))\n",
    "    worthless_sent = []\n",
    "    for x in range(no_sent):\n",
    "        for y in range(no_sent):\n",
    "            adotb = np.dot(count_matrix[d][x], count_matrix[d][y])\n",
    "            a = LA.norm(count_matrix[d][x])\n",
    "            b = LA.norm(count_matrix[d][y])\n",
    "            if(x==y):\n",
    "                sim[x,y] = (1-mu)/no_sent\n",
    "            else:\n",
    "                sim[x,y] = mu*(adotb/(a*b)) + (1-mu)/no_sent\n",
    "    \n",
    "    denom = np.sum(sim,axis=1)\n",
    "    for x in range(no_sent):\n",
    "        sim[x,:] = sim[x,:] * 1/denom[x]\n",
    "        \n",
    "    # Markov's process\n",
    "    v = np.array(score_sents)\n",
    "    \n",
    "    while(True):\n",
    "        v_dash = v.dot(sim)\n",
    "        if(LA.norm(v_dash - v) < 0.000000001):\n",
    "            break\n",
    "        v = v_dash\n",
    "    \n",
    "    alpha = {}\n",
    "    beta = {}\n",
    "    redundacy = sim;\n",
    "\n",
    "    # ILP selection\n",
    "    solver = pywraplp.Solver('SolveAssignmentProblemMIP', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n",
    "\n",
    "    for i in range(no_sent):\n",
    "        alpha[i] = solver.BoolVar('alpha[%i]' % i)\n",
    "\n",
    "    for i in range(no_sent):\n",
    "        for j in range(i+1,no_sent):\n",
    "            beta[i,j] = solver.BoolVar('beta[%i,%i]' %(i,j))\n",
    "\n",
    "\n",
    "    solver.Maximize(solver.Sum(alpha[i]*lam*v[i] for i in range(no_sent)) - solver.Sum([beta[i,j]*(1-lam)*redundacy[i,j] for j in range(i+1,no_sent) for i in range(no_sent)]))\n",
    "\n",
    "    # constraints\n",
    "    solver.Add( solver.Sum([alpha[i] for i in range(no_sent)]) <= k )\n",
    "\n",
    "    for i in range(no_sent):\n",
    "        for j in range(i+1,no_sent):\n",
    "            solver.Add( beta[i,j] <= alpha[i])\n",
    "            solver.Add( beta[i,j] <= alpha[j])\n",
    "            solver.Add( alpha[i] + alpha[j] <= 1 + beta[i,j])\n",
    "\n",
    "    sol = solver.Solve()\n",
    "    \n",
    "    alpha_sol = [alpha[i].solution_value() for i in range(no_sent)]\n",
    "    top_sents = []\n",
    "    for i in range(no_sent):\n",
    "        if alpha_sol[i]==1:\n",
    "            top_sents.append(copy_corpus[d][i])\n",
    "    top_corpus[d] = top_sents\n",
    "    \n",
    "    if(d%10 == 0):\n",
    "        print(d,end=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X.AUTHID</th>\n",
       "      <th>FEA1</th>\n",
       "      <th>FEA2</th>\n",
       "      <th>FEA3</th>\n",
       "      <th>FEA_NRC1</th>\n",
       "      <th>FEA_NRC2</th>\n",
       "      <th>FEA_NRC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'http://blog.searsholdings.com/leadership-vie...</td>\n",
       "      <td>SHC Speaks by As Chief HR Officer and co-e spo...</td>\n",
       "      <td>This year, SHC partnered with , and to marry f...</td>\n",
       "      <td>I was joined by dozens of SHC associates and f...</td>\n",
       "      <td>SHC Speaks by As Chief HR Officer and co-e spo...</td>\n",
       "      <td>This year, SHC partnered with , and to marry f...</td>\n",
       "      <td>I was joined by dozens of SHC associates and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'https://atyourservice.blogs.xerox.com/catego...</td>\n",
       "      <td>When you look at your Xerox device and see the...</td>\n",
       "      <td>Recently new support videos were posted to the...</td>\n",
       "      <td>Xerox and Xerox and Design are trademarks of X...</td>\n",
       "      <td>When you look at your Xerox device and see the...</td>\n",
       "      <td>Recently new support videos were posted to the...</td>\n",
       "      <td>Xerox and Xerox and Design are trademarks of X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'http://blogs.zebra.com/whitepaper-label'</td>\n",
       "      <td>; March 17, 2015 at 8:03 AM ;Product identifi...</td>\n",
       "      <td>It is critical that the labels you utilize are...</td>\n",
       "      <td>;In this paper, Zebra provides guidance on how...</td>\n",
       "      <td>; March 17, 2015 at 8:03 AM ;Product identifi...</td>\n",
       "      <td>It is critical that the labels you utilize are...</td>\n",
       "      <td>;In this paper, Zebra provides guidance on how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'https://connect.blogs.xerox.com/author/lauri...</td>\n",
       "      <td>Writer and PR Consultant Email LinkedIn Twitte...</td>\n",
       "      <td>By By Laurie Riedman, PR Consultant Throughout...</td>\n",
       "      <td>Xerox and Xerox and Design are trademarks of X...</td>\n",
       "      <td>Writer and PR Consultant Email LinkedIn Twitte...</td>\n",
       "      <td>By By Laurie Riedman, PR Consultant Throughout...</td>\n",
       "      <td>Xerox and Xerox and Design are trademarks of X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'http://blog.searsholdings.com/2016/07/page/2/'</td>\n",
       "      <td>For our Danbury Sears store, this meant that w...</td>\n",
       "      <td>On Saturday, July 2, Sears celebrated the gran...</td>\n",
       "      <td>More than 20 Sears associates volunteered with...</td>\n",
       "      <td>For our Danbury Sears store, this meant that w...</td>\n",
       "      <td>On Saturday, July 2, Sears celebrated the gran...</td>\n",
       "      <td>More than 20 Sears associates volunteered with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            X.AUTHID  \\\n",
       "0  b'http://blog.searsholdings.com/leadership-vie...   \n",
       "1  b'https://atyourservice.blogs.xerox.com/catego...   \n",
       "2         b'http://blogs.zebra.com/whitepaper-label'   \n",
       "3  b'https://connect.blogs.xerox.com/author/lauri...   \n",
       "4   b'http://blog.searsholdings.com/2016/07/page/2/'   \n",
       "\n",
       "                                                FEA1  \\\n",
       "0  SHC Speaks by As Chief HR Officer and co-e spo...   \n",
       "1  When you look at your Xerox device and see the...   \n",
       "2   ; March 17, 2015 at 8:03 AM ;Product identifi...   \n",
       "3  Writer and PR Consultant Email LinkedIn Twitte...   \n",
       "4  For our Danbury Sears store, this meant that w...   \n",
       "\n",
       "                                                FEA2  \\\n",
       "0  This year, SHC partnered with , and to marry f...   \n",
       "1  Recently new support videos were posted to the...   \n",
       "2  It is critical that the labels you utilize are...   \n",
       "3  By By Laurie Riedman, PR Consultant Throughout...   \n",
       "4  On Saturday, July 2, Sears celebrated the gran...   \n",
       "\n",
       "                                                FEA3  \\\n",
       "0  I was joined by dozens of SHC associates and f...   \n",
       "1  Xerox and Xerox and Design are trademarks of X...   \n",
       "2  ;In this paper, Zebra provides guidance on how...   \n",
       "3  Xerox and Xerox and Design are trademarks of X...   \n",
       "4  More than 20 Sears associates volunteered with...   \n",
       "\n",
       "                                            FEA_NRC1  \\\n",
       "0  SHC Speaks by As Chief HR Officer and co-e spo...   \n",
       "1  When you look at your Xerox device and see the...   \n",
       "2   ; March 17, 2015 at 8:03 AM ;Product identifi...   \n",
       "3  Writer and PR Consultant Email LinkedIn Twitte...   \n",
       "4  For our Danbury Sears store, this meant that w...   \n",
       "\n",
       "                                            FEA_NRC2  \\\n",
       "0  This year, SHC partnered with , and to marry f...   \n",
       "1  Recently new support videos were posted to the...   \n",
       "2  It is critical that the labels you utilize are...   \n",
       "3  By By Laurie Riedman, PR Consultant Throughout...   \n",
       "4  On Saturday, July 2, Sears celebrated the gran...   \n",
       "\n",
       "                                            FEA_NRC3  \n",
       "0  I was joined by dozens of SHC associates and f...  \n",
       "1  Xerox and Xerox and Design are trademarks of X...  \n",
       "2  ;In this paper, Zebra provides guidance on how...  \n",
       "3  Xerox and Xerox and Design are trademarks of X...  \n",
       "4  More than 20 Sears associates volunteered with...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data[\"FEA_NRC1\"] = [s[0] for s in top_corpus]\n",
    "top_data[\"FEA_NRC2\"] = [s[1] for s in top_corpus]\n",
    "top_data[\"FEA_NRC3\"] = [s[2] for s in top_corpus]\n",
    "                     \n",
    "top_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data.to_csv('topdata_featbase.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
